import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor

# Load the training data
train_data = pd.read_csv('/content/sample_data/train.csv')

# Extract features and target variable from the training data
X_train = train_data.drop(['id', 'yield'], axis=1)
y_train = train_data['yield']

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Preprocessing pipeline
preprocessing_pipeline = make_pipeline(StandardScaler())

# Apply preprocessing to training and validation data
X_train_preprocessed = preprocessing_pipeline.fit_transform(X_train)
X_val_preprocessed = preprocessing_pipeline.transform(X_val)

# Feature selection using Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_preprocessed, y_train)

# Feature importance analysis
feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf.feature_importances_})
selected_features = feature_importances[feature_importances['Importance'] >= threshold]['Feature']

# Select features from training and validation data
X_train_selected = X_train[selected_features]
X_val_selected = X_val[selected_features]

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_leaf': [1, 2, 3]
}

rf_tuned = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(rf_tuned, param_grid, cv=5)
grid_search.fit(X_train_selected, y_train)
best_params
